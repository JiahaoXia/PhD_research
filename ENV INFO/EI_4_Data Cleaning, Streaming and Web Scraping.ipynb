{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developed by Roger Wang (rq.wang@rutgers.edu)\n",
    "\n",
    "Reference: https://www.pythonforbeginners.com/api/how-to-use-the-hacker-news-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning using Pandas\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with missing values\n",
    "\n",
    "raw_data = {'first_name': ['Jason', np.nan, 'Tina', 'Jake', 'Amy'], \n",
    "        'last_name': ['Miller', np.nan, 'Ali', 'Milner', 'Cooze'], \n",
    "        'age': [42, np.nan, 36, 24, 73], \n",
    "        'sex': ['m', np.nan, 'f', 'm', 'f'], \n",
    "        'preTestScore': [4, np.nan, np.nan, 2, 3],\n",
    "        'postTestScore': [25, np.nan, np.nan, 62, 70]}\n",
    "df = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'sex', 'preTestScore', 'postTestScore'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing observations\n",
    "\n",
    "df_no_missing = df.dropna()\n",
    "df_no_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where all cells in that row is NA\n",
    "\n",
    "df_cleaned = df.dropna(how='all')\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column full of missing values\n",
    "\n",
    "df['location'] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column if they only contain missing values\n",
    "\n",
    "df.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that contain less than five observations\n",
    "\n",
    "df.dropna(thresh=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing data with zeros\n",
    "\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing in preTestScore with the mean value of preTestScore\n",
    "\n",
    "df[\"preTestScore\"].fillna(df[\"preTestScore\"].mean(), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing in postTestScore with each sex’s mean value of postTestScore\n",
    "\n",
    "df[\"postTestScore\"].fillna(df.groupby(\"sex\")[\"postTestScore\"].transform(\"mean\"), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select some raws but ignore the missing data points\n",
    "\n",
    "# Select the rows of df where age is not NaN and sex is not NaN\n",
    "df[df['age'].notnull() & df['sex'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge:\n",
    "--\n",
    "1.\tImport raritan_river_data.csv file (http://mosaic.njaes.rutgers.edu/raritan-river/download/ boathouse site) into a pandas dataframe and show its first 5 rows\n",
    "2.\tFirst, drop all the rows that contain less than 8 non null values\n",
    "3.\tThen, remove the columns that contain NaN values\n",
    "4.\tReplace the column names of “buoy” and “ox” by “site” and “oxygen”\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "5.\tRe-order the data by the depth.\t\n",
    "6.\tShow the top 5 rows with the highest oxygen value.\n",
    "7.\tPlot a histogram of the frequency of depth with 20 bins.\n",
    "8.\tPlot a scatter with oxygen versus temperature, size of 12 in by 6 in, and a title of “oxygen v.s. temperature”\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API examples:\n",
    "    --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "geopy is a Python 2 and 3 client for several popular geocoding web services.\n",
    "\n",
    "geopy makes it easy for Python developers to locate the coordinates of addresses, cities, countries, and landmarks across the globe using third-party geocoders and other data sources.\n",
    "\n",
    "geopy includes geocoder classes for the OpenStreetMap Nominatim, Google Geocoding API (V3), and many other geocoding services. The full list is available on the Geocoders doc section. Geocoder classes are located in geopy.geocoders.\n",
    "\n",
    "geopy is tested against CPython (versions 2.7, 3.4, 3.5, 3.6, 3.7), PyPy, and PyPy3. geopy does not and will not support CPython 2.6.\n",
    "\n",
    "© geopy contributors 2006-2018 (see AUTHORS) under the MIT License.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Install using pip with:\n",
    "\n",
    "pip install geopy --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To geolocate a query to an address and coordinates:\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\n",
    "location = geolocator.geocode(\"175 5th Avenue NYC\")\n",
    "print(location.address)\n",
    "print('*'*50)\n",
    "print((location.latitude, location.longitude))\n",
    "print('*'*50)\n",
    "print(location.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the address corresponding to a set of coordinates:\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\n",
    "location = geolocator.reverse(\"52.509669, 13.376294\")\n",
    "print(location.address)\n",
    "print('*'*50)\n",
    "print((location.latitude, location.longitude))\n",
    "print('*'*50)\n",
    "print(location.raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Distance\n",
    "\n",
    "Geopy can calculate geodesic distance between two points using the geodesic distance or the great-circle distance, with a default of the geodesic distance available as the function geopy.distance.distance.\n",
    "\n",
    "Here's an example usage of the geodesic distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "newport_ri = (41.49008, -71.312796)\n",
    "cleveland_oh = (41.499498, -81.695391)\n",
    "print(geodesic(newport_ri, cleveland_oh).miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using great-circle distance:\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "newport_ri = (41.49008, -71.312796)\n",
    "cleveland_oh = (41.499498, -81.695391)\n",
    "print(great_circle(newport_ri, cleveland_oh).miles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further look inside\n",
    "--\n",
    "Source: app.dataquest.io\n",
    "\n",
    "Organizations host their APIs on Web servers. When you type www.google.com in your browser's address bar, your computer is actually asking the www.google.com server for a Web page, which it then returns to your browser.\n",
    "\n",
    "APIs work much the same way, except instead of your Web browser asking for a Web page, your program asks for data. The API usually returns this data in JavaScript Object Notation (JSON) format. We'll discuss JSON more later on in this mission.\n",
    "\n",
    "We make an API request to the Web server we want to get data from. The server then replies and sends it to us. In Python, we use the requests library to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different types of requests. The most common is a GET request, which we use to retrieve data. We'll explore the other types in later missions.\n",
    "\n",
    "We can use a simple GET request to retrieve information from the OpenNotify API.\n",
    "\n",
    "OpenNotify has several API endpoints. An endpoint is a server route for retrieving specific data from an API. For example, the /comments endpoint on the reddit API might retrieve information about comments, while the /users endpoint might retrieve data about users.\n",
    "\n",
    "The first endpoint we'll look at on OpenNotify is the iss-now.json endpoint. This endpoint gets the current latitude and longitude position of the ISS. A data set wouldn't be a great fit for this task because the information changes often, and involves some calculation on the server.\n",
    "\n",
    "Check out the complete list of OpenNotify endpoints.\n",
    "\n",
    "We've imported requests for you already, so please avoid doing it again in this mission. Importing requests will overwrite some of the custom API logic we've developed for answer checking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction\n",
    "\n",
    "The server will send a status code indicating the success or failure of your request. You can get the status code of the response from response.status_code.\n",
    "Assign the status code to the variable status_code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# Make a get request to get the latest position of the ISS from the OpenNotify API.\n",
    "response = requests.get(\"http://api.open-notify.org/iss-now.json\")\n",
    "status_code = response.status_code\n",
    "print(status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request we just made returned a status code of 200. Web servers return status codes every time they receive an API request. A status code provides information about what happened with a request. Here are some codes that are relevant to GET requests:\n",
    "\n",
    "200 - Everything went okay, and the server returned a result (if any).<br>\n",
    "301 - The server is redirecting you to a different endpoint. This can happen when a company switches domain names, or an endpoint's name has changed.<br>\n",
    "401 - The server thinks you're not authenticated. This happens when you don't send the right credentials to access an API (we'll talk about this in a later mission).<br>\n",
    "400 - The server thinks you made a bad request. This can happen when you don't send the information the API requires to process your request, among other things.<br>\n",
    "403 - The resource you're trying to access is forbidden; you don't have the right permissions to see it.<br>\n",
    "404 - The server didn't find the resource you tried to access.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction\n",
    "Make a GET request to http://api.open-notify.org/iss-pass.\n",
    "\n",
    "Assign the status code of the response to status_code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer below.\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass\")\n",
    "status_code = response.status_code\n",
    "print(status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iss-pass wasn't a valid endpoint, so the API's server sent us a 404 status code in response. We forgot to add .json at the end, like the API documentation tells us to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction\n",
    "Make a GET request to http://api.open-notify.org/iss-pass.json.\n",
    "\n",
    "Assign the status code of the response to status_code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer below.\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\")\n",
    "status_code = response.status_code\n",
    "print(status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that in the last example, we got a 400 status code, which indicates a bad request. If you look at the documentation for the OpenNotify API, we see that the ISS Pass endpoint requires two parameters.\n",
    "\n",
    "This endpoint returns the next time the ISS will pass over a given location on the Earth.\n",
    "\n",
    "To request this information, we'll need to pass the coordinates for a specific location to the API. We do this by passing in two parameters, latitude and longitude.\n",
    "\n",
    "To accomplish this, we can add an optional keyword argument, params, to our request. In this case, we need to pass in two parameters:\n",
    "\n",
    "lat - The latitude of the location\n",
    "lon - The longitude of the location\n",
    "We can make a dictionary that contains these parameters, and then pass them into the function.\n",
    "\n",
    "We can also do the same thing directly by adding the query parameters to the url, like this:\n",
    "\n",
    "http://api.open-notify.org/iss-pass.json?lat=40.71&lon=-74\n",
    "\n",
    "It's almost always preferable to set up the parameters as a dictionary, because the requests library we mentioned earlier takes care of certain issues, like properly formatting the query parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters we want to pass to the API.\n",
    "# This is the latitude and longitude of New York City.\n",
    "parameters = {\"lat\": 40.71, \"lon\": -74}\n",
    "\n",
    "# Make a get request with the parameters.\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=parameters)\n",
    "\n",
    "# Print the content of the response (the data the server returned)\n",
    "print(response.content)\n",
    "\n",
    "# This gets the same data as the command above\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json?lat=40.71&lon=-74\")\n",
    "print(response.content)\n",
    "parameters = {\"lat\": 37.78, \"lon\": -122.41}\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=parameters)\n",
    "content = response.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the content of the API response we received earlier was a string. Strings are the way we pass information back and forth through APIs, but it's hard to get the information we want out of them. How do we know how to decode the string we receive and work with it in Python?\n",
    "\n",
    "Luckily, there's a format we call JSON. We mentioned it earlier in the mission. This format encodes data structures like lists and dictionaries as strings to ensure that machines can read them easily. JSON is the primary format for sending and receiving data through APIs.\n",
    "\n",
    "Python offers great support for JSON through its json library. We can convert lists and dictionaries to JSON, and vice versa. Our ISS Pass data, for example, is a dictionary encoded as a string in JSON format.\n",
    "\n",
    "The JSON library has two main methods:\n",
    "\n",
    "dumps -- Takes in a Python object, and converts it to a string\n",
    "loads -- Takes a JSON string, and converts it to a Python object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction\n",
    "\n",
    "Use the JSON function loads to convert fast_food_franchise_string to a Python object.\n",
    "\n",
    "Assign the resulting Python object to fast_food_franchise_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of fast food chains.\n",
    "best_food_chains = [\"Taco Bell\", \"Shake Shack\", \"Chipotle\"]\n",
    "print(type(best_food_chains))\n",
    "\n",
    "# Import the JSON library.\n",
    "import json\n",
    "\n",
    "# Use json.dumps to convert best_food_chains to a string.\n",
    "best_food_chains_string = json.dumps(best_food_chains)\n",
    "print(type(best_food_chains_string))\n",
    "\n",
    "# Convert best_food_chains_string back to a list.\n",
    "print(type(json.loads(best_food_chains_string)))\n",
    "\n",
    "# Make a dictionary\n",
    "fast_food_franchise = {\n",
    "    \"Subway\": 24722,\n",
    "    \"McDonalds\": 14098,\n",
    "    \"Starbucks\": 10821,\n",
    "    \"Pizza Hut\": 7600\n",
    "}\n",
    "\n",
    "# We can also dump a dictionary to a string and load it.\n",
    "fast_food_franchise_string = json.dumps(fast_food_franchise)\n",
    "print(type(fast_food_franchise_string))\n",
    "fast_food_franchise_2 = json.loads(fast_food_franchise_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the content of a response as a Python object by using the .json() method on the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction\n",
    "\n",
    "Get the duration value of the ISS' first pass over San Francisco and assign the value to first_pass_duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same request we did two screens ago.\n",
    "parameters = {\"lat\": 37.78, \"lon\": -122.41}\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=parameters)\n",
    "\n",
    "# Get the response data as a Python object.  Verify that it's a dictionary.\n",
    "json_data = response.json()\n",
    "print(type(json_data))\n",
    "print(json_data)\n",
    "first_pass_duration = json_data[\"response\"][0][\"duration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The server sends more than a status code and the data when it generates a response. It also sends metadata containing information on how it generated the data and how to decode it. This information appears in the response headers. We can access it using the .headers property that responses have.\n",
    "\n",
    "The headers will appear as a dictionary. For now, the content-type within the headers is the most important key. It tells us the format of the response, and how to decode it. For the OpenNotify API, the format is JSON, which is why we could decode it with JSON earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction\n",
    "Get content-type from response.headers.\n",
    "\n",
    "Assign the content type to the content_type variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers is a dictionary\n",
    "print(response.headers)\n",
    "content_type = response.headers[\"content-type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenNotify has one more API endpoint, astros.json. It tells us how many people are currently in space. You can find the format of the responses here.\n",
    "\n",
    "Because we implement our own version of this API on our servers, this number will most likely be different from the current number (when you try to access the original API outside of Dataquest)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction\n",
    "Find how many people are currently in space.\n",
    "\n",
    "Assign the result to in_space_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the API here.\n",
    "response = requests.get(\"http://api.open-notify.org/astros.json\")\n",
    "json_data = response.json()\n",
    "\n",
    "in_space_count = json_data[\"number\"]\n",
    "in_space_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "Syntax\n",
    "Accessing the content of the data the server returns:\n",
    "\n",
    "response.content\n",
    "\n",
    "Importing the JSON library:\n",
    "\n",
    "import json\n",
    "\n",
    "Getting the content of a response as a Python object:\n",
    "\n",
    "response.json()\n",
    "\n",
    "Accessing the information on how the server generated the data, and how to decode the data:\n",
    "\n",
    "response.headers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge:\n",
    "--\n",
    "    1. Read the tutorial: http://abdulbaqi.io/2017/09/13/Wdi/\n",
    "    2. Retrieve the USA average annual temperatures of 1901-2012\n",
    "    3. Plot the temperature trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Web Scraping Tutorial using BeautifulSoup\n",
    "--\n",
    "source: dataquest.io\n",
    "\n",
    "When performing data science tasks, it's common to want to use data found on the internet. You'll usually be able to access this data in csv format, or via an Application Programming Interface (API). However, there are times when the data you want can only be accessed as part of a web page. In cases like this, you'll want to use a technique called web scraping to get the data from the web page into a format you can work with in your analysis.\n",
    "\n",
    "In this tutorial, we'll show you how to perform web scraping using Python 3 and the BeautifulSoup library. We'll be scraping weather forecasts from the National Weather Service, and then analyzing them using the Pandas library.\n",
    "\n",
    "We'll be scraping weather forecasts from the National Weather Service site.\n",
    "\n",
    "Before we get started, if you're looking for more background on APIs or the csv format, you might want to check out our Dataquest courses on APIs or data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The components of a web page\n",
    "When we visit a web page, our web browser makes a request to a web server. This request is called a GET request, since we’re getting files from the server. The server then sends back files that tell our browser how to render the page for us. The files fall into a few main types:\n",
    "\n",
    "HTML — contain the main content of the page.\n",
    "CSS — add styling to make the page look nicer.\n",
    "JS — Javascript files add interactivity to web pages.\n",
    "Images — image formats, such as JPG and PNG allow web pages to show pictures.\n",
    "After our browser receives all the files, it renders the page and displays it to us. There’s a lot that happens behind the scenes to render a page nicely, but we don’t need to worry about most of it when we’re web scraping. When we perform web scraping, we’re interested in the main content of the web page, so we look at the HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML\n",
    "HyperText Markup Language (HTML) is a language that web pages are created in. HTML isn’t a programming language, like Python — instead, it’s a markup language that tells a browser how to layout content. HTML allows you to do similar things to what you do in a word processor like Microsoft Word — make text bold, create paragraphs, and so on. Because HTML isn’t a programming language, it isn’t nearly as complex as Python.\n",
    "\n",
    "Let’s take a quick tour through HTML so we know enough to scrape effectively. HTML consists of elements called tags. The most basic tag is the <html> tag. This tag tells the web browser that everything inside of it is HTML. We can make a simple HTML document just using this tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "<html>\n",
    "    \n",
    "</html>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven’t added any content to our page yet, so if we viewed our HTML document in a web browser, we wouldn’t see anything:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right inside an html tag, we put two other tags, the head tag, and the body tag. The main content of the web page goes into the body tag. The head tag contains data about the title of the page, and other information that generally isn’t useful in web scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "<body>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still haven’t added any content to our page (that goes inside the body tag), so we again won’t see anything:\n",
    "\n",
    "You may have noticed above that we put the head and body tags inside the html tag. In HTML, tags are nested, and can go inside other tags.\n",
    "\n",
    "We’ll now add our first content to the page, in the form of the p tag. The p tag defines a paragraph, and any text inside the tag is shown as a separate paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "          <p>\n",
    "            Here's a paragraph of text!\n",
    "        </p>\n",
    "        <p>\n",
    "            Here's a second paragraph of text!\n",
    "        </p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s how this will look:\n",
    "\n",
    "Here’s a paragraph of text!\n",
    "\n",
    "Here’s a second paragraph of text!\n",
    "\n",
    "Tags have commonly used names that depend on their position in relation to other tags:\n",
    "\n",
    "child — a child is a tag inside another tag. So the two p tags above are both children of the body tag.\n",
    "parent — a parent is the tag another tag is inside. Above, the html tag is the parent of the body tag.\n",
    "sibiling — a sibiling is a tag that is nested inside the same parent as another tag. For example, head and body are siblings, since they’re both inside html. Both p tags are siblings, since they’re both inside body.\n",
    "We can also add properties to HTML tags that change their behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p>\n",
    "            Here's a paragraph of text!\n",
    "            <a href=\"https://www.dataquest.io\">Learn Data Science Online</a>\n",
    "        </p>\n",
    "        <p>\n",
    "            Here's a second paragraph of text!\n",
    "            <a href=\"https://www.python.org\">Python</a>        </p>\n",
    "    </body></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s how this will look:\n",
    "\n",
    "Here’s a paragraph of text! Learn Data Science Online\n",
    "\n",
    "Here’s a second paragraph of text! Python\n",
    "\n",
    "In the above example, we added two a tags. a tags are links, and tell the browser to render a link to another web page. The href property of the tag determines where the link goes.\n",
    "\n",
    "a and p are extremely common html tags. Here are a few others:\n",
    "\n",
    "div — indicates a division, or area, of the page.\n",
    "b — bolds any text inside.\n",
    "i — italicizes any text inside.\n",
    "table — creates a table.\n",
    "form — creates an input form.\n",
    "For a full list of tags, look here.\n",
    "\n",
    "Before we move into actual web scraping, let’s learn about the class and id properties. These special properties give HTML elements names, and make them easier to interact with when we’re scraping. One element can have multiple classes, and a class can be shared between elements. Each element can only have one id, and an id can only be used once on a page. Classes and ids are optional, and not all elements will have them.\n",
    "\n",
    "We can add classes and ids to our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"bold-paragraph\">\n",
    "            Here's a paragraph of text!\n",
    "            <a href=\"https://www.dataquest.io\" id=\"learn-link\">Learn Data Science Online</a>\n",
    "        </p>\n",
    "        <p class=\"bold-paragraph extra-large\">\n",
    "            Here's a second paragraph of text!\n",
    "            <a href=\"https://www.python.org\" class=\"extra-large\">Python</a>\n",
    "        </p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s how this will look:\n",
    "\n",
    "Here’s a paragraph of text! Learn Data Science Online\n",
    "\n",
    "Here’s a second paragraph of text! Python\n",
    "\n",
    "As you can see, adding classes and ids doesn’t change how the tags are rendered at all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping Example\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "# Here, we're just importing both Beautiful Soup and the Requests library\n",
    "page_link = 'https://cee.rutgers.edu/'\n",
    "# this is the url that we've already determined is safe and legal to scrape from.\n",
    "page_response = requests.get(page_link, timeout=5)\n",
    "# here, we fetch the content from the url, using the requests library\n",
    "page_content = BeautifulSoup(page_response.content, \"html.parser\")\n",
    "#we use the html parser to parse the url content and store it in a variable.\n",
    "textContent = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page_content.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=page_content\n",
    "\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(id=\"menu-696-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One common task is extracting all the URLs found within a page’s <a> tags:\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another common task is extracting all the text from a page:\n",
    "\n",
    "print(soup.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge:\n",
    "    --\n",
    "    1. Open the website at https://airnow.gov/\n",
    "    2. Find the webpage showing the local airquality at Rutgers\n",
    "    3. Scrape the air quality data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
